{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function normalize is taken from [Prabhanshu Attri et al](https://keras.io/examples/timeseries/timeseries_weather_forecasting/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, train_split):\n",
    "    data_mean = np.nanmean(data[:train_split], axis=0)\n",
    "    data_std = np.nanstd(data[:train_split], axis=0)\n",
    "    return (data - data_mean) / data_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations are recorded every hour. \n",
    "We are tracking data from past 720 timestamps (30 days) or 336 timestamps (2 weeks). This data will be used to predict the rainfall amount after 24 timestamps (1 day).\n",
    "All the following functions are used to fit a specific neural network to the data. These functions take as input the data and the number of outputs of 1 or more layer. The other parameters have a default values but they can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prediction_1LSTM(data, a, split_fraction=0.715, step=1, past=720, future=24, learning_rate=0.001, batch_size=256, epochs=10):\n",
    "    train_split = int(split_fraction * int(data.shape[0]))\n",
    "    train_data = data.loc[0 : train_split - 1]\n",
    "    val_data = data.loc[train_split:]\n",
    "    start = past + future #start of y lablels\n",
    "    end = start + train_split #end of y labels\n",
    "    x_end = len(val_data) - past - future#index of the last x to consider for the validation set\n",
    "\n",
    "    x_train = train_data.drop('rain', axis=1)\n",
    "    y_train = data.iloc[start:end,0]\n",
    "\n",
    "    x_val = val_data.drop('rain', axis=1).iloc[:x_end]\n",
    "    y_val = val_data.iloc[start:,0]\n",
    "\n",
    "    sequence_length = int(past/step)\n",
    "    dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,#x\n",
    "    y_train,#y\n",
    "    sequence_length=sequence_length,#number of x needed to predict the y\n",
    "    sampling_rate=step,#how many timestamp to skip\n",
    "    batch_size=batch_size,#number of timeseries in each batch\n",
    "    )\n",
    "\n",
    "    dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x_val,\n",
    "        y_val,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    for batch in dataset_train.take(1):\n",
    "        inputs, targets = batch\n",
    "        \n",
    "    #NN\n",
    "    inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "    lstm_out = keras.layers.LSTM(a)(inputs)#long short term memory layer\n",
    "    outputs = keras.layers.Dense(1)(lstm_out)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")#Adam for gradient descent\n",
    "    print(model.summary())\n",
    "    \n",
    "    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "    modelckpt_callback = keras.callbacks.ModelCheckpoint( #allows to save model parameters for each epoch such that the best \n",
    "        #can be found (no risk of losing efficiency in successive epochs)\n",
    "        monitor=\"val_loss\",\n",
    "        filepath=\"model_checkpoint.h5\",\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        dataset_train,\n",
    "        epochs=epochs,\n",
    "        validation_data=dataset_val,\n",
    "        callbacks=[es_callback, modelckpt_callback],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_1LSTM_2D(data, a, b, split_fraction=0.715, step=1, past=720, future=24, learning_rate=0.001, batch_size=256, epochs=10):\n",
    "    train_split = int(split_fraction * int(data.shape[0]))\n",
    "    train_data = data.loc[0 : train_split - 1]\n",
    "    val_data = data.loc[train_split:]\n",
    "    start = past + future #start of y lablels\n",
    "    end = start + train_split #end of y labels\n",
    "    x_end = len(val_data) - past - future#index of the last x to consider for the validation set\n",
    "\n",
    "    x_train = train_data.drop('rain', axis=1)\n",
    "    y_train = data.iloc[start:end,0]\n",
    "\n",
    "    x_val = val_data.drop('rain', axis=1).iloc[:x_end]\n",
    "    y_val = val_data.iloc[start:,0]\n",
    "\n",
    "    sequence_length = int(past/step)\n",
    "    dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,#x\n",
    "    y_train,#y\n",
    "    sequence_length=sequence_length,#number of x neede to predict the y\n",
    "    sampling_rate=step,#how many timestamp to skip\n",
    "    batch_size=batch_size,#number of timeseries in each batch\n",
    "    )\n",
    "\n",
    "    dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x_val,\n",
    "        y_val,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    for batch in dataset_train.take(1):\n",
    "        inputs, targets = batch\n",
    "        \n",
    "    #NN\n",
    "    inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "    lstm_out = keras.layers.LSTM(a)(inputs)#long short term memory layer\n",
    "    dense_out = keras.layers.Dense(b)(lstm_out)\n",
    "    outputs = keras.layers.Dense(1)(dense_out)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")#Adam for gradient descent\n",
    "    print(model.summary())\n",
    "    \n",
    "    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "    modelckpt_callback = keras.callbacks.ModelCheckpoint( #allows to save model parameters for each epoch such that the best \n",
    "        #can be found (no risk of losing efficiency in successive epochs)\n",
    "        monitor=\"val_loss\",\n",
    "        filepath=\"model_checkpoint.h5\",\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        dataset_train,\n",
    "        epochs=epochs,\n",
    "        validation_data=dataset_val,\n",
    "        callbacks=[es_callback, modelckpt_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_DENSE(data, split_fraction=0.715, step=1, past=720, future=24, learning_rate=0.001, batch_size=256, epochs=10):\n",
    "    train_split = int(split_fraction * int(data.shape[0]))\n",
    "    train_data = data.loc[0 : train_split - 1]\n",
    "    val_data = data.loc[train_split:]\n",
    "    start = past + future #start of y lablels\n",
    "    end = start + train_split #end of y labels\n",
    "    x_end = len(val_data) - past - future#index of the last x to consider for the validation set\n",
    "\n",
    "    x_train = train_data.drop('rain', axis=1)\n",
    "    y_train = data.iloc[start:end,0]\n",
    "\n",
    "    x_val = val_data.drop('rain', axis=1).iloc[:x_end]\n",
    "    y_val = val_data.iloc[start:,0]\n",
    "\n",
    "    sequence_length = int(past/step)\n",
    "    dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,#x\n",
    "    y_train,#y\n",
    "    sequence_length=sequence_length,#number of x neede to predict the y\n",
    "    sampling_rate=step,#how many timestamp to skip\n",
    "    batch_size=batch_size,#number of timeseries in each batch\n",
    "    )\n",
    "\n",
    "    dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x_val,\n",
    "        y_val,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    for batch in dataset_train.take(1):\n",
    "        inputs, targets = batch\n",
    "        \n",
    "    #NN  \n",
    "    inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "    dense1 = keras.layers.Dense(50, activation=\"relu\")(inputs)\n",
    "    dense2 = keras.layers.Dense(20, activation=\"relu\")(dense1)\n",
    "    dense3 = keras.layers.Dense(5, activation=\"relu\")(dense2)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(dense3)\n",
    "\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")#Adam for gradient descent\n",
    "    print(model.summary())\n",
    "    \n",
    "    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "    modelckpt_callback = keras.callbacks.ModelCheckpoint( #allows to save model parameters for each epoch such that the best \n",
    "        #can be found (no risk of losing efficiency in successive epochs)\n",
    "        monitor=\"val_loss\",\n",
    "        filepath=\"model_checkpoint.h5\",\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        dataset_train,\n",
    "        epochs=epochs,\n",
    "        validation_data=dataset_val,\n",
    "        callbacks=[es_callback, modelckpt_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_1GRU(data, a, split_fraction=0.715, step=1, past=720, future=24, learning_rate=0.001, batch_size=256, epochs=10):\n",
    "    train_split = int(split_fraction * int(data.shape[0]))\n",
    "    train_data = data.loc[0 : train_split - 1]\n",
    "    val_data = data.loc[train_split:]\n",
    "    start = past + future #start of y lablels\n",
    "    end = start + train_split #end of y labels\n",
    "    x_end = len(val_data) - past - future#index of the last x to consider for the validation set\n",
    "\n",
    "    x_train = train_data.drop('rain', axis=1)\n",
    "    y_train = data.iloc[start:end,0]\n",
    "\n",
    "    x_val = val_data.drop('rain', axis=1).iloc[:x_end]\n",
    "    y_val = val_data.iloc[start:,0]\n",
    "\n",
    "    sequence_length = int(past/step)\n",
    "    dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,#x\n",
    "    y_train,#y\n",
    "    sequence_length=sequence_length,#number of x neede to predict the y\n",
    "    sampling_rate=step,#how many timestamp to skip\n",
    "    batch_size=batch_size,#number of timeseries in each batch\n",
    "    )\n",
    "\n",
    "    dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x_val,\n",
    "        y_val,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    for batch in dataset_train.take(1):\n",
    "        inputs, targets = batch\n",
    "        \n",
    "    #NN\n",
    "    inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "    lstm_out = keras.layers.GRU(a)(inputs)#gru layer\n",
    "    outputs = keras.layers.Dense(1)(lstm_out)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")#Adam for gradient descent\n",
    "    print(model.summary())\n",
    "    \n",
    "    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "    modelckpt_callback = keras.callbacks.ModelCheckpoint( #allows to save model parameters for each epoch such that the best \n",
    "        #can be found (no risk of losing efficiency in successive epochs)\n",
    "        monitor=\"val_loss\",\n",
    "        filepath=\"model_checkpoint.h5\",\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        dataset_train,\n",
    "        epochs=epochs,\n",
    "        validation_data=dataset_val,\n",
    "        callbacks=[es_callback, modelckpt_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_1GRU_2D(data, a, b, split_fraction=0.715, step=1, past=720, future=24, learning_rate=0.001, batch_size=256, epochs=10):\n",
    "    train_split = int(split_fraction * int(data.shape[0]))\n",
    "    train_data = data.loc[0 : train_split - 1]\n",
    "    val_data = data.loc[train_split:]\n",
    "    start = past + future #start of y lablels\n",
    "    end = start + train_split #end of y labels\n",
    "    x_end = len(val_data) - past - future#index of the last x to consider for the validation set\n",
    "\n",
    "    x_train = train_data.drop('rain', axis=1)\n",
    "    y_train = data.iloc[start:end,0]\n",
    "\n",
    "    x_val = val_data.drop('rain', axis=1).iloc[:x_end]\n",
    "    y_val = val_data.iloc[start:,0]\n",
    "\n",
    "    sequence_length = int(past/step)\n",
    "    dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,#x\n",
    "    y_train,#y\n",
    "    sequence_length=sequence_length,#number of x neede to predict the y\n",
    "    sampling_rate=step,#how many timestamp to skip\n",
    "    batch_size=batch_size,#number of timeseries in each batch\n",
    "    )\n",
    "\n",
    "    dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x_val,\n",
    "        y_val,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    for batch in dataset_train.take(1):\n",
    "        inputs, targets = batch\n",
    "        \n",
    "    #NN\n",
    "    inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "    lstm_out = keras.layers.GRU(a)(inputs)\n",
    "    dense_out = keras.layers.Dense(b)(lstm_out)\n",
    "    outputs = keras.layers.Dense(1)(dense_out)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")#Adam for gradient descent\n",
    "    print(model.summary())\n",
    "    \n",
    "    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "    modelckpt_callback = keras.callbacks.ModelCheckpoint( #allows to save model parameters for each epoch such that the best \n",
    "        #can be found (no risk of losing efficiency in successive epochs)\n",
    "        monitor=\"val_loss\",\n",
    "        filepath=\"model_checkpoint.h5\",\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        dataset_train,\n",
    "        epochs=epochs,\n",
    "        validation_data=dataset_val,\n",
    "        callbacks=[es_callback, modelckpt_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_1GRU_1LSTM_1DENSE(data, a, b, c, split_fraction=0.715, step=1, past=720, future=24, learning_rate=0.001, batch_size=256, epochs=10):\n",
    "    train_split = int(split_fraction * int(data.shape[0]))\n",
    "    train_data = data.loc[0 : train_split - 1]\n",
    "    val_data = data.loc[train_split:]\n",
    "    start = past + future #start of y lablels\n",
    "    end = start + train_split #end of y labels\n",
    "    x_end = len(val_data) - past - future#index of the last x to consider for the validation set\n",
    "\n",
    "    x_train = train_data.drop('rain', axis=1)\n",
    "    y_train = data.iloc[start:end,0]\n",
    "\n",
    "    x_val = val_data.drop('rain', axis=1).iloc[:x_end]\n",
    "    y_val = val_data.iloc[start:,0]\n",
    "\n",
    "    sequence_length = int(past/step)\n",
    "    dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,#x\n",
    "    y_train,#y\n",
    "    sequence_length=sequence_length,#number of x neede to predict the y\n",
    "    sampling_rate=step,#how many timestamp to skip\n",
    "    batch_size=batch_size,#number of timeseries in each batch\n",
    "    )\n",
    "\n",
    "    dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x_val,\n",
    "        y_val,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    for batch in dataset_train.take(1):\n",
    "        inputs, targets = batch\n",
    "        \n",
    "    #NN\n",
    "    inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "    gru_out = keras.layers.GRU(a, return_sequences=True)(inputs)\n",
    "    lstm_out = keras.layers.LSTM(b)(gru_out)\n",
    "    dense_out = keras.layers.Dense(c)(lstm_out)\n",
    "    outputs = keras.layers.Dense(1)(dense_out)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")#Adam for gradient descent\n",
    "    print(model.summary())\n",
    "    \n",
    "    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "    modelckpt_callback = keras.callbacks.ModelCheckpoint( #allows to save model parameters for each epoch such that the best \n",
    "        #can be found (no risk of losing efficiency in successive epochs)\n",
    "        monitor=\"val_loss\",\n",
    "        filepath=\"model_checkpoint.h5\",\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        dataset_train,\n",
    "        epochs=epochs,\n",
    "        validation_data=dataset_val,\n",
    "        callbacks=[es_callback, modelckpt_callback],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
